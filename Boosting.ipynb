{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Boosting**:\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ **1. What is Boosting, and how does it differ from Bagging in ensemble learning?**\n",
        "\n",
        "- **Boosting** is an ensemble technique that **combines multiple weak learners sequentially** to form a strong learner. Each new model corrects the errors made by the previous ones by focusing more on misclassified instances.\n",
        "\n",
        "- **Difference from Bagging**:\n",
        "\n",
        "| Aspect             | Bagging                  | Boosting                               |\n",
        "| ------------------ | ------------------------ | -------------------------------------- |\n",
        "| **Model Training** | Parallel                 | Sequential                             |\n",
        "| **Goal**           | Reduce variance          | Reduce bias and variance               |\n",
        "| **Data Weighting** | Uniform sample weighting | Higher weights to misclassified points |\n",
        "| **Examples**       | Random Forest            | AdaBoost, Gradient Boosting, XGBoost   |\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ **2. How does Boosting reduce bias and variance in machine learning models?**\n",
        "\n",
        "* **Bias Reduction**: Boosting corrects errors of the prior learners by sequentially adding models trained on the residuals or misclassifications.\n",
        "* **Variance Reduction**: The ensemble effect stabilizes predictions and reduces overfitting, especially with regularized boosting variants like XGBoost.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ **3. How does Boosting iteratively improve model performance by focusing on misclassified data points?**\n",
        "\n",
        "* In each iteration:\n",
        "\n",
        "  1. The model identifies instances that were misclassified.\n",
        "  2. It **increases the weight or attention** on these errors.\n",
        "  3. The next model in the sequence is trained to correct those errors.\n",
        "  4. This process continues until a stopping criterion is met (like max iterations or minimal improvement).\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¸ **AdaBoost (Adaptive Boosting)**\n",
        "\n",
        "### **4. What is AdaBoost, and how does it work to improve model accuracy?**\n",
        "\n",
        "**AdaBoost** stands for **Adaptive Boosting** â€” it boosts the performance of weak classifiers (usually decision stumps) by focusing on the mistakes made in earlier rounds.\n",
        "\n",
        "#### ðŸ”§ How It Works:\n",
        "\n",
        "* **Initial model** is trained on the dataset with equal weights for all samples.\n",
        "* **Subsequent models** focus more on **misclassified samples** by increasing their weights.\n",
        "* Each weak learnerâ€™s prediction is **weighted** based on its accuracy.\n",
        "* Final prediction is a **weighted majority vote** (classification) or **weighted sum** (regression) of all weak learners.\n",
        "\n",
        "ðŸ“Œ **Key Idea**: It adapts by giving more importance to difficult-to-classify examples.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. How does AdaBoost assign weights to weak learners during the training process?**\n",
        "\n",
        "In AdaBoost:\n",
        "\n",
        "* Let error rate of weak learner $h_t$ be $\\epsilon_t$.\n",
        "* The **learnerâ€™s weight** is computed as:\n",
        "\n",
        "$$\n",
        "\\alpha_t = \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_t}{\\epsilon_t} \\right)\n",
        "$$\n",
        "\n",
        "* Learners with **lower error** get **higher $\\alpha_t$** (more influence).\n",
        "* Sample weights are updated for the next iteration:\n",
        "\n",
        "$$\n",
        "w_{i}^{(t+1)} = w_i^{(t)} \\cdot e^{-\\alpha_t y_i h_t(x_i)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $y_i$ = true label,\n",
        "* $h_t(x_i)$ = prediction by current learner.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Compare AdaBoost with Gradient Boosting in terms of computational complexity and performance.**\n",
        "\n",
        "| Feature                 | AdaBoost                                         | Gradient Boosting                             |\n",
        "| ----------------------- | ------------------------------------------------ | --------------------------------------------- |\n",
        "| **Error Focus**         | Reweights data points based on misclassification | Fits residuals (errors) from previous model   |\n",
        "| **Loss Function**       | Exponential loss                                 | Any differentiable loss (e.g., MSE, log loss) |\n",
        "| **Complexity**          | Lower (uses decision stumps often)               | Higher (fits full decision trees)             |\n",
        "| **Performance**         | Good on clean data                               | Better on complex patterns or large datasets  |\n",
        "| **Robustness to Noise** | Less robust                                      | More robust with regularization               |\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Implement AdaBoost using scikit-learn for a classification task**\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# AdaBoost Classifier\n",
        "model = AdaBoostClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **8. How would you tune hyperparameters for AdaBoost to improve model accuracy?**\n",
        "\n",
        "ðŸ”§ **Key hyperparameters** to tune:\n",
        "\n",
        "| Parameter        | Description                                   |\n",
        "| ---------------- | --------------------------------------------- |\n",
        "| `n_estimators`   | Number of weak learners                       |\n",
        "| `learning_rate`  | Weight applied to each classifier (shrinkage) |\n",
        "| `base_estimator` | Type of weak learner (e.g., DecisionTree)     |\n",
        "\n",
        "ðŸ“ˆ **Tuning Strategy**:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 1.0],\n",
        "    'base_estimator': [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2)]\n",
        "}\n",
        "\n",
        "model = AdaBoostClassifier(random_state=42)\n",
        "grid = GridSearchCV(model, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **9. Explain how AdaBoost handles noisy data and outliers in a dataset.**\n",
        "\n",
        "* **Not robust to noise/outliers**: AdaBoost increases weights of all misclassified points, including **irreducible errors** like noise or mislabeled data.\n",
        "\n",
        "* As a result, it can **overfit** to outliers.\n",
        "* **Mitigation Strategies**:\n",
        "\n",
        "  * Early stopping\n",
        "\n",
        "  * Limiting number of estimators\n",
        "\n",
        "  * Using robust base learners\n",
        "\n",
        "  * Using Gradient Boosting or XGBoost with regularization\n",
        "\n",
        "---\n",
        "\n",
        "Excellent! Letâ€™s continue with **Gradient Boosting** â€” key for **Machine Learning Engineer** and **Data Scientist** roles at top companies like **Swiggy, Zomato, Amazon, Flipkart, Google**, and **Meesho**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¸ **Gradient Boosting**\n",
        "\n",
        "### **10. What is Gradient Boosting, and how does it differ from AdaBoost?**\n",
        "\n",
        "**Gradient Boosting** is an ensemble method that builds models sequentially, like AdaBoost, but it generalizes boosting by using **gradient descent to minimize any differentiable loss function**.\n",
        "\n",
        "#### ðŸ”„ Key Differences:\n",
        "\n",
        "| Feature            | AdaBoost                                   | Gradient Boosting                            |\n",
        "| ------------------ | ------------------------------------------ | -------------------------------------------- |\n",
        "| **Loss Function**  | Exponential loss                           | Any differentiable loss (e.g., MSE, LogLoss) |\n",
        "| **Error Handling** | Increases weights on misclassified samples | Fits to residuals from previous models       |\n",
        "| **Optimization**   | No gradient used                           | Uses gradient of loss function               |\n",
        "| **Robustness**     | Less robust to noise                       | More robust with regularization              |\n",
        "\n",
        "---\n",
        "\n",
        "### **11. Explain the role of loss functions in gradient boosting algorithms.**\n",
        "\n",
        "Loss functions **measure the difference** between predicted and actual values. Gradient Boosting uses the **gradient (derivative)** of this loss function to guide the training of the next weak learner.\n",
        "\n",
        "ðŸ“˜ Examples of loss functions:\n",
        "\n",
        "* **Regression**:\n",
        "\n",
        "  * Mean Squared Error (MSE): $L = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2$\n",
        "* **Classification**:\n",
        "\n",
        "  * Log Loss: $L = -\\sum y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)$\n",
        "\n",
        "In each step, the model fits the **negative gradient** (residuals) of the loss function â€” making each iteration a move toward minimizing the error.\n",
        "\n",
        "---\n",
        "\n",
        "### **12. Compare the convergence speed of Gradient Boosting with AdaBoost and XGBoost.**\n",
        "\n",
        "| Algorithm             | Convergence Speed     | Regularization    | Parallelization | Notes                         |\n",
        "| --------------------- | --------------------- | ----------------- | --------------- | ----------------------------- |\n",
        "| **AdaBoost**          | Fast (for small data) | Limited           | âŒ               | May overfit on noisy data     |\n",
        "| **Gradient Boosting** | Moderate              | Partial           | âŒ               | Slower, but better control    |\n",
        "| **XGBoost**           | Faster                | Extensive (L1/L2) | âœ…               | Highly optimized and scalable |\n",
        "\n",
        "ðŸ§  **Takeaway**: XGBoost > Gradient Boosting > AdaBoost in most real-world ML pipelines.\n",
        "\n",
        "---\n",
        "\n",
        "### **13. Implement Gradient Boosting using scikit-learn for a regression problem**\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate sample regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Train Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **14. How would you select the optimal learning rate for a gradient boosting model?**\n",
        "\n",
        "* The **learning rate** (also called **shrinkage**) controls how much each tree contributes to the final model.\n",
        "\n",
        "ðŸ“‰ **Lower learning rate â†’ better generalization but slower convergence.**\n",
        "\n",
        "ðŸ” **Tuning Strategy**:\n",
        "\n",
        "1. Try values like `0.01`, `0.05`, `0.1`, `0.2`.\n",
        "2. Couple with higher `n_estimators`.\n",
        "3. Use `GridSearchCV` or `RandomizedSearchCV`.\n",
        "\n",
        "ðŸ§ª **Example**:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 4]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(GradientBoostingRegressor(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **15. What metrics would you use to evaluate the performance of a gradient boosting model on a regression task?**\n",
        "\n",
        "ðŸ” **Popular Evaluation Metrics**:\n",
        "\n",
        "| Metric                   | Description                                   |\n",
        "| ------------------------ | --------------------------------------------- |\n",
        "| **MSE / RMSE**           | Penalizes larger errors                       |\n",
        "| **MAE**                  | More robust to outliers                       |\n",
        "| **RÂ² Score (R-squared)** | Proportion of variance explained by the model |\n",
        "\n",
        "ðŸ“Œ Example in Python:\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
        "print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¸ **XGBoost (Extreme Gradient Boosting)**\n",
        "\n",
        "### **16. What is XGBoost, and how does it improve over traditional Gradient Boosting?**\n",
        "\n",
        "**XGBoost** is an optimized implementation of gradient boosting designed for performance and speed.\n",
        "\n",
        "ðŸš€ **Key Improvements Over Gradient Boosting**:\n",
        "\n",
        "| Feature                     | Traditional GBM | XGBoost                                  |\n",
        "| --------------------------- | --------------- | ---------------------------------------- |\n",
        "| **Regularization**          | No or limited   | L1 (Lasso) & L2 (Ridge) included         |\n",
        "| **Speed**                   | Slower          | 10â€“100x faster (tree pruning, histogram) |\n",
        "| **Parallel Processing**     | âŒ               | âœ… Built-in                               |\n",
        "| **Handling Missing Values** | Manual          | âœ… Auto-learn best split direction        |\n",
        "| **Early Stopping**          | Manual          | âœ… Built-in                               |\n",
        "| **Sparsity Awareness**      | âŒ               | âœ… Handles sparse data efficiently        |\n",
        "\n",
        "---\n",
        "\n",
        "### **17. How does XGBoost handle missing data during training?**\n",
        "\n",
        "* XGBoost **automatically learns** the best direction (left/right) to send missing values in a split by evaluating loss reduction.\n",
        "* No need for imputation!\n",
        "\n",
        "ðŸ“˜ Internally:\n",
        "\n",
        "> It creates default directions for missing values per tree node based on training loss minimization.\n",
        "\n",
        "---\n",
        "\n",
        "### **18. Explain the importance of regularization in XGBoost and its effect on model performance.**\n",
        "\n",
        "Regularization helps prevent **overfitting**, especially on noisy data or small datasets.\n",
        "\n",
        "ðŸ›¡ï¸ **Two types of regularization used**:\n",
        "\n",
        "* **L1 (Î±)**: Induces sparsity â€” useful for feature selection.\n",
        "* **L2 (Î»)**: Penalizes large weights â€” encourages smoother models.\n",
        "\n",
        "ðŸ“ **Regularized Objective Function**:\n",
        "\n",
        "$$\n",
        "Obj = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\left[ \\gamma T_k + \\frac{1}{2} \\lambda \\sum_j w_{k,j}^2 \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\gamma$: Penalty on the number of leaves,\n",
        "* $\\lambda$: L2 regularization on leaf weights.\n",
        "\n",
        "ðŸ“Œ Helps in:\n",
        "\n",
        "* Controlling model complexity\n",
        "* Reducing overfitting\n",
        "* Improving generalization\n",
        "\n",
        "---\n",
        "\n",
        "### **19. Implement XGBoost using Python for a classification problem**\n",
        "\n",
        "```python\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric='logloss')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **20. How would you tune the hyperparameters of an XGBoost model to improve its accuracy?**\n",
        "\n",
        "ðŸ”§ **Commonly tuned hyperparameters**:\n",
        "\n",
        "| Parameter                 | Description                                    |\n",
        "| ------------------------- | ---------------------------------------------- |\n",
        "| `n_estimators`            | Number of boosting rounds                      |\n",
        "| `max_depth`               | Tree depth (controls model complexity)         |\n",
        "| `learning_rate`           | Shrinkage step (lower = better generalization) |\n",
        "| `subsample`               | Fraction of data used per tree (e.g. 0.8)      |\n",
        "| `colsample_bytree`        | Fraction of features per tree (e.g. 0.8)       |\n",
        "| `gamma`                   | Minimum loss reduction for a split             |\n",
        "| `reg_alpha`, `reg_lambda` | L1 and L2 regularization                       |\n",
        "\n",
        "~ **Tuning Example** (Randomized Search):\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [100, 200],\n",
        "    'subsample': [0.7, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "random_search = RandomizedSearchCV(model, param_grid, cv=3, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best Params:\", random_search.best_params_)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **21. Compare the performance of XGBoost with Random Forest for a given classification problem.**\n",
        "\n",
        "| Feature                | Random Forest               | XGBoost                                   |\n",
        "| ---------------------- | --------------------------- | ----------------------------------------- |\n",
        "| **Training**           | Parallel trees              | Sequential trees                          |\n",
        "| **Speed**              | Fast, parallel              | Fast (optimized sequential)               |\n",
        "| **Regularization**     | No L1/L2                    | Yes â€” L1 & L2                             |\n",
        "| **Overfitting**        | Higher risk                 | Lower risk with regularization            |\n",
        "| **Handling Imbalance** | Needs sampling/class weight | Built-in handling with scale\\_pos\\_weight |\n",
        "| **Interpretability**   | Better                      | Complex, but SHAP values help             |\n",
        "\n",
        "~ **XGBoost usually outperforms RF** on structured/tabular data, especially with:\n",
        "\n",
        "* Hyperparameter tuning\n",
        "\n",
        "* Feature selection\n",
        "\n",
        "* Imbalanced datasets\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J6Wj31FdeLxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load data and split into training and testing sets\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [100, 200],\n",
        "    'subsample': [0.7, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "random_search = RandomizedSearchCV(model, param_grid, cv=3, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best Params:\", random_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVMu8C9KBH6e",
        "outputId": "b7c71898-9578-4a8d-ecd8-a07e8310c68c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric='logloss')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSthE5_EEj-s",
        "outputId": "45f89e88-6f58-4e84-fa35-4f0f6efe5089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9532163742690059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor  # Import GradientBoostingRegressor\n",
        "\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 4]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(GradientBoostingRegressor(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
        "print(\"R2 Score:\", r2_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adIM3h6QEoMa",
        "outputId": "09794065-b552-4258-e649-24ded56e3700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
            "MAE: 0.04678362573099415\n",
            "R2 Score: 0.798941798941799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate sample regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Train Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcKoZ0cjFXyR",
        "outputId": "c215f313-47e4-4bc6-a29f-206189bd7e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 2442.207481824917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier # Import AdaBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer # Import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load breast cancer data again to ensure classification data is used\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 1.0],\n",
        "    # Change 'base_estimator' to 'estimator' for scikit-learn >= 1.0\n",
        "    'estimator': [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2)]\n",
        "}\n",
        "\n",
        "model = AdaBoostClassifier(random_state=42)\n",
        "grid = GridSearchCV(model, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-oWzlBxIuqt",
        "outputId": "d068a573-e9c9-4b4e-f8a0-bee25fdb0e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'estimator': DecisionTreeClassifier(max_depth=1), 'learning_rate': 1.0, 'n_estimators': 50}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# AdaBoost Classifier\n",
        "model = AdaBoostClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT9NTweGK9LE",
        "outputId": "956e0f3d-0b7f-4059-f698-af31d26e4996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7966666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nW22IwexLR_C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}